# Speech-Emotion-Analyzer-
Featured Enriched Speech Emotion Recognition using Mel Frequency Cepstral Coefficients
This project proposes a robust deep learning framework for SER, leveraging CNNs and Mel Frequency Cepstral Coefficients (MFCCs) as input features. The model is evaluated using well-known datasets—CREMA-D, RAVDESS, SAVEE, and TESS—to ensure diversity in emotional expressions and speaker characteristics.

Key Contributions:

Customized CNN Architecture: The study introduces a tailored CNN design that effectively captures local temporal and spectral features from MFCCs, enhancing emotion detection accuracy.
Enhanced Feature Extraction: The use of 13-dimensional MFCCs approximates human auditory perception, offering a compact and meaningful representation of emotional speech.
Data Augmentation Techniques: Strategies such as noise addition, pitch shifting, and time-stretching are applied to improve the model’s robustness and ability to generalize across various acoustic conditions.
Comprehensive Evaluation: The model’s performance is rigorously tested on diverse datasets to validate its effectiveness and generalization capabilities.
High-Accuracy Emotion Classification: The proposed approach demonstrates superior accuracy compared to traditional methods and achieves competitive results against other deep learning models in classifying six emotion categories.
